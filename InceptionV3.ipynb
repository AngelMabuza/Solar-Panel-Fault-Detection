{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d8a214-3a3a-4d63-abce-847b3bc64b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import concatenate\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.layers import Conv2D , MaxPool2D , Input , GlobalAveragePooling2D ,AveragePooling2D, Dense , Dropout ,Activation, Flatten , BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2937201c",
   "metadata": {},
   "source": [
    "#### load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35272365-fa20-4595-9c5b-07563a02c7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()\n",
    "CLASS_NAMES= ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbc76c2",
   "metadata": {},
   "source": [
    "#### Set training, test and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a474e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the dataset\n",
    "validation_images, validation_labels = train_images[:5000], train_labels[:5000]\n",
    "train_images, train_labels = train_images[5000:], train_labels[5000:]\n",
    "\n",
    "# Building tensorflow datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "validation_ds = tf.data.Dataset.from_tensor_slices((validation_images, validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452520c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f77e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(image, label):\n",
    "    # Normalize images to have a mean of 0 and standard deviation of 1\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    image = tf.image.resize(image, (299,299))\n",
    "    return image, label\n",
    "\n",
    "train_ds_size = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "test_ds_size = tf.data.experimental.cardinality(test_ds).numpy()\n",
    "validation_ds_size = tf.data.experimental.cardinality(validation_ds).numpy()\n",
    "\n",
    "train_ds = (train_ds\n",
    "                  .map(process_images)\n",
    "                  .shuffle(buffer_size=train_ds_size)\n",
    "                  .batch(batch_size=32, drop_remainder=True))\n",
    "test_ds = (test_ds\n",
    "                  .map(process_images)\n",
    "                  .shuffle(buffer_size=train_ds_size)\n",
    "                  .batch(batch_size=32, drop_remainder=True))\n",
    "validation_ds = (validation_ds\n",
    "                  .map(process_images)\n",
    "                  .shuffle(buffer_size=train_ds_size)\n",
    "                  .batch(batch_size=32, drop_remainder=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b003e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def InceptionV3():\n",
    "    input_layer = Input(shape=(299 , 299 , 3))\n",
    "    \n",
    "    x = StemBlock(input_layer)\n",
    "    \n",
    "    x = InceptionBlock_A(prev_layer = x ,nbr_kernels = 32)\n",
    "    x = InceptionBlock_A(prev_layer = x ,nbr_kernels = 64)\n",
    "    x = InceptionBlock_A(prev_layer = x ,nbr_kernels = 64)\n",
    "    \n",
    "    x = ReductionBlock_A(prev_layer = x )\n",
    "    \n",
    "    x = InceptionBlock_B(prev_layer = x  , nbr_kernels = 128)\n",
    "    x = InceptionBlock_B(prev_layer = x , nbr_kernels = 160)\n",
    "    x = InceptionBlock_B(prev_layer = x , nbr_kernels = 160)\n",
    "    x = InceptionBlock_B(prev_layer = x , nbr_kernels = 192)\n",
    "\n",
    "    Aux = auxiliary_classifier(prev_Layer = x)\n",
    "    \n",
    "    x = ReductionBlock_B(prev_layer = x)\n",
    "    \n",
    "    x = InceptionBlock_C(prev_layer = x)\n",
    "    x = InceptionBlock_C(prev_layer = x)\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(units=2048, activation='relu') (x)\n",
    "    x = Dropout(rate = 0.2) (x)\n",
    "    x = Dense(units=1000, activation='softmax') (x)\n",
    "    model = Model(inputs = input_layer , outputs = [x , Aux] , name = 'Inception-V3')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def conv_with_Batch_Normalisation(prev_layer , nbr_kernels , filter_Size , strides =(1,1) , padding = 'same'):\n",
    "    x = Conv2D(filters=nbr_kernels, kernel_size = filter_Size, strides=strides , padding=padding)(prev_layer)\n",
    "    x = BatchNormalization(axis=3)(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def StemBlock(prev_layer):\n",
    "    x = conv_with_Batch_Normalisation(prev_layer, nbr_kernels = 32, filter_Size=(3,3) , strides=(2,2))\n",
    "    x = conv_with_Batch_Normalisation(x, nbr_kernels = 32, filter_Size=(3,3))\n",
    "    x = conv_with_Batch_Normalisation(x, nbr_kernels = 64, filter_Size=(3,3))\n",
    "    x = MaxPool2D(pool_size=(3,3) , strides=(2,2)) (x)\n",
    "    x = conv_with_Batch_Normalisation(x, nbr_kernels = 80, filter_Size=(1,1))\n",
    "    x = conv_with_Batch_Normalisation(x, nbr_kernels = 192, filter_Size=(3,3))\n",
    "    x = MaxPool2D(pool_size=(3,3) , strides=(2,2)) (x)\n",
    "    return x    \n",
    "\n",
    "def InceptionBlock_A(prev_layer  , nbr_kernels):\n",
    "    \n",
    "    branch1 = conv_with_Batch_Normalisation(prev_layer, nbr_kernels = 64, filter_Size = (1,1))\n",
    "    branch1 = conv_with_Batch_Normalisation(branch1, nbr_kernels=96, filter_Size=(3,3))\n",
    "    branch1 = conv_with_Batch_Normalisation(branch1, nbr_kernels=96, filter_Size=(3,3))\n",
    "    \n",
    "    branch2 = conv_with_Batch_Normalisation(prev_layer, nbr_kernels=48, filter_Size=(1,1))\n",
    "    branch2 = conv_with_Batch_Normalisation(branch2, nbr_kernels=64, filter_Size=(3,3)) # may be 3*3\n",
    "    \n",
    "    branch3 = AveragePooling2D(pool_size=(3,3) , strides=(1,1) , padding='same') (prev_layer)\n",
    "    branch3 = conv_with_Batch_Normalisation(branch3, nbr_kernels = nbr_kernels, filter_Size = (1,1))\n",
    "    branch4 = conv_with_Batch_Normalisation(prev_layer, nbr_kernels=64, filter_Size=(1,1))\n",
    "    \n",
    "    output = concatenate([branch1 , branch2 , branch3 , branch4], axis=3)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def InceptionBlock_B(prev_layer , nbr_kernels):\n",
    "    \n",
    "    branch1 = conv_with_Batch_Normalisation(prev_layer, nbr_kernels = nbr_kernels, filter_Size = (1,1))\n",
    "    branch1 = conv_with_Batch_Normalisation(branch1, nbr_kernels = nbr_kernels, filter_Size = (7,1))\n",
    "    branch1 = conv_with_Batch_Normalisation(branch1, nbr_kernels = nbr_kernels, filter_Size = (1,7))\n",
    "    branch1 = conv_with_Batch_Normalisation(branch1, nbr_kernels = nbr_kernels, filter_Size = (7,1))    \n",
    "    branch1 = conv_with_Batch_Normalisation(branch1, nbr_kernels = 192, filter_Size = (1,7))\n",
    "    \n",
    "    branch2 = conv_with_Batch_Normalisation(prev_layer, nbr_kernels = nbr_kernels, filter_Size = (1,1))\n",
    "    branch2 = conv_with_Batch_Normalisation(branch2, nbr_kernels = nbr_kernels, filter_Size = (1,7))\n",
    "    branch2 = conv_with_Batch_Normalisation(branch2, nbr_kernels = 192, filter_Size = (7,1))\n",
    "    \n",
    "    branch3 = AveragePooling2D(pool_size=(3,3) , strides=(1,1) , padding ='same') (prev_layer)\n",
    "    branch3 = conv_with_Batch_Normalisation(branch3, nbr_kernels = 192, filter_Size = (1,1))\n",
    "    branch4 = conv_with_Batch_Normalisation(prev_layer, nbr_kernels = 192, filter_Size = (1,1))\n",
    "    \n",
    "    output = concatenate([branch1 , branch2 , branch3 , branch4], axis = 3)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def InceptionBlock_C(prev_layer):\n",
    "    \n",
    "    branch1 = conv_with_Batch_Normalisation(prev_layer, nbr_kernels = 448, filter_Size = (1,1))\n",
    "    branch1 = conv_with_Batch_Normalisation(branch1, nbr_kernels = 384, filter_Size = (3,3))\n",
    "    branch1_1 = conv_with_Batch_Normalisation(branch1, nbr_kernels = 384, filter_Size = (1,3))    \n",
    "    branch1_2 = conv_with_Batch_Normalisation(branch1, nbr_kernels = 384, filter_Size = (3,1))\n",
    "    branch1 = concatenate([branch1_1 , branch1_2], axis = 3)\n",
    "    \n",
    "    branch2 = conv_with_Batch_Normalisation(prev_layer, nbr_kernels = 384, filter_Size = (1,1))\n",
    "    branch2_1 = conv_with_Batch_Normalisation(branch2, nbr_kernels = 384, filter_Size = (1,3))\n",
    "    branch2_2 = conv_with_Batch_Normalisation(branch2, nbr_kernels = 384, filter_Size = (3,1))\n",
    "    branch2 = concatenate([branch2_1 , branch2_2], axis = 3)\n",
    "    \n",
    "    branch3 = AveragePooling2D(pool_size=(3,3) , strides=(1,1) , padding='same')(prev_layer)\n",
    "    branch3 = conv_with_Batch_Normalisation(branch3, nbr_kernels = 192, filter_Size = (1,1))\n",
    "    branch4 = conv_with_Batch_Normalisation(prev_layer, nbr_kernels = 320, filter_Size = (1,1))\n",
    "    \n",
    "    output = concatenate([branch1 , branch2 , branch3 , branch4], axis = 3)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def ReductionBlock_A(prev_layer):\n",
    "    \n",
    "    branch1 = conv_with_Batch_Normalisation(prev_layer, nbr_kernels = 64, filter_Size = (1,1))\n",
    "    branch1 = conv_with_Batch_Normalisation(branch1, nbr_kernels = 96, filter_Size = (3,3))\n",
    "    branch1 = conv_with_Batch_Normalisation(branch1, nbr_kernels = 96, filter_Size = (3,3) , strides=(2,2) ) #, padding='valid'\n",
    "    \n",
    "    branch2 = conv_with_Batch_Normalisation(prev_layer, nbr_kernels = 384, filter_Size=(3,3) , strides=(2,2) )\n",
    "    \n",
    "    branch3 = MaxPool2D(pool_size=(3,3) , strides=(2,2) , padding='same')(prev_layer)\n",
    "    \n",
    "    output = concatenate([branch1 , branch2 , branch3], axis = 3)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def ReductionBlock_B(prev_layer):\n",
    "    \n",
    "    branch1 = conv_with_Batch_Normalisation(prev_layer, nbr_kernels = 192, filter_Size = (1,1))\n",
    "    branch1 = conv_with_Batch_Normalisation(branch1, nbr_kernels = 192, filter_Size = (1,7))\n",
    "    branch1 = conv_with_Batch_Normalisation(branch1, nbr_kernels = 192, filter_Size = (7,1))\n",
    "    branch1 = conv_with_Batch_Normalisation(branch1, nbr_kernels = 192, filter_Size = (3,3) , strides=(2,2) , padding = 'valid')\n",
    "    \n",
    "    branch2 = conv_with_Batch_Normalisation(prev_layer, nbr_kernels = 192, filter_Size = (1,1) )\n",
    "    branch2 = conv_with_Batch_Normalisation(branch2, nbr_kernels = 320, filter_Size = (3,3) , strides=(2,2) , padding='valid' )\n",
    "\n",
    "    branch3 = MaxPool2D(pool_size=(3,3) , strides=(2,2) )(prev_layer)\n",
    "    \n",
    "    output = concatenate([branch1 , branch2 , branch3], axis = 3)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def auxiliary_classifier(prev_Layer):\n",
    "    x = AveragePooling2D(pool_size=(5,5) , strides=(3,3)) (prev_Layer)\n",
    "    x = conv_with_Batch_Normalisation(x, nbr_kernels = 128, filter_Size = (1,1))\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(units = 768, activation='relu') (x)\n",
    "    x = Dropout(rate = 0.2) (x)\n",
    "    x = Dense(units = 1000, activation='softmax') (x)\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c88e8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Incpetion V3 Model Architecture\n",
    "\n",
    "model = InceptionV3()\n",
    "\n",
    "# Compiling the Model\n",
    "model.compile(optimizer='adam', loss=keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "# Checking Model Summary\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
